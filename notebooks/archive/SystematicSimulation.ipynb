{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import datajoint as dj\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from matplotlib import pyplot as plt\n",
    "from attorch.train import early_stopping\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from numpy.linalg import inv\n",
    "from itertools import chain, product, count\n",
    "from tqdm import tqdm\n",
    "from v1_likelihood.utils import list_hash, set_seed\n",
    "from v1_likelihood.models import Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting eywalker@10.28.0.34:3306\n"
     ]
    }
   ],
   "source": [
    "schema = dj.schema('edgar_cd_ml_sim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_ones(x):\n",
    "    return np.concatenate([x, np.ones([x.shape[0], 1])], axis=1)\n",
    "\n",
    "\n",
    "def binnify(x, center=0, delta=1, nbins=61, clip=True):\n",
    "    \"\"\"\n",
    "    Bin the dat into bins, with center bin at `center`. Each bin has width `delta`\n",
    "    and you will have equal number of bins to the left and to the right of the center bin.\n",
    "    The left most bin starts at bin number and the last bin at `nbins`-1. If `clip`=True,\n",
    "    then data falling out of the bins would be assigned bin number `-1` to indicate that it\n",
    "    is out of the range. Otherwise, the data would be assigned to the nearest edge bin. A data point\n",
    "    x would fall into bin i if  bin_i_left <= x < bin_i_right\n",
    "\n",
    "    Args:\n",
    "        x: data to bin\n",
    "        center: center of the bins\n",
    "        delta: width of each bin\n",
    "        nbins: number of bins\n",
    "        clip: whether to clip data falling out of bin range. Defaults to True\n",
    "\n",
    "    Returns:\n",
    "        (xv, p) - xv is an array of bin centers and thus has length nbins. p is the bin assignment of\n",
    "            each data point in x and thus len(p) == len(x).\n",
    "    \"\"\"\n",
    "    p = np.round((x - center) / delta) + (nbins // 2)\n",
    "    if clip:\n",
    "        out = (p < 0) | (p >= nbins)\n",
    "        p[out] = -1\n",
    "    else:\n",
    "        p[p < 0] = 0\n",
    "        p[p >= nbins] = nbins -1\n",
    "\n",
    "    xv = (np.arange(nbins) - nbins//2) * delta + center\n",
    "    return xv, p\n",
    "\n",
    "\n",
    "@schema\n",
    "class StimulusSession(dj.Lookup):\n",
    "    definition = \"\"\"\n",
    "    stim_id: int   # simulation id\n",
    "    stim_seed: int # simulation seed\n",
    "    ---\n",
    "    train_size: int    # trainset size\n",
    "    valid_size: int    # validset size\n",
    "    test_size: int     # testset size\n",
    "    \"\"\"\n",
    "    contents = [(0, 12345, 800, 200, 200)]\n",
    "\n",
    "    sigma_a = 3\n",
    "    sigma_b = 15\n",
    "\n",
    "    def get_stimulus(self, key=None):\n",
    "        key = key or {}\n",
    "        key = (self & key).fetch1()\n",
    "\n",
    "        np.random.seed(key['stim_seed'])\n",
    "        train_size, valid_size, test_size = key['train_size'], key['valid_size'], key['test_size']\n",
    "        total_trials = train_size + valid_size + test_size\n",
    "        sv = np.random.choice([self.sigma_a, self.sigma_b], size=total_trials)\n",
    "        ori = np.random.randn(total_trials) * sv\n",
    "\n",
    "        # split them into training set, validation set, and test set\n",
    "        return ori[:train_size], ori[train_size:(train_size+valid_size)], ori[(train_size+valid_size):]\n",
    "\n",
    "    @classmethod\n",
    "    def log_prior(cls, pv):\n",
    "        sigmaA = cls.sigma_a\n",
    "        sigmaB = cls.sigma_b\n",
    "        return np.log(np.exp(- pv ** 2 / 2 / sigmaA ** 2) / sigmaA + np.exp(- pv ** 2 / 2 / sigmaB ** 2) / sigmaB)\n",
    "\n",
    "\n",
    "@schema\n",
    "class GaussTuningSet(dj.Manual):\n",
    "    definition = \"\"\"\n",
    "    tuning_set_id: int    # tuning set id\n",
    "    ---\n",
    "    n_cells: int  # number of cells\n",
    "    centers: longblob # ceneters of tuning curves\n",
    "    amps: longblob   # amplitudes of tuning curves\n",
    "    widths: longblob # widths of tuning curves\n",
    "    \"\"\"\n",
    "    \n",
    "    def fill(self):\n",
    "        # add first set of tuned neurons\n",
    "        key = {\n",
    "            'tuning_set_id': 0,\n",
    "            'n_cells': 96,\n",
    "        }\n",
    "        key['centers'] = np.linspace(-40, 40, key['n_cells'])\n",
    "        key['amps'] = 6 * np.ones(key['n_cells'])\n",
    "        key['widths'] = 21 * np.ones(key['n_cells'])\n",
    "        self.insert1(key, skip_duplicates=True)\n",
    "\n",
    "    def get_f(self, key=None):\n",
    "        \"\"\"\n",
    "        Return a function that would return mean population response for stimuli.\n",
    "        Returns n_neurons x n_stimuli matrix\n",
    "        \"\"\"\n",
    "        key = key or {}\n",
    "        centers, amps, widths = (self & key).fetch1('centers', 'amps', 'widths')\n",
    "        centers = centers[:, None]\n",
    "        amps = amps[:, None]\n",
    "        widths = widths[:, None]\n",
    "        def f(stim):\n",
    "            return np.exp(-(stim - centers)**2 / 2 / widths**2) * amps\n",
    "\n",
    "        return f\n",
    "\n",
    "@schema\n",
    "class ResponseGain(dj.Lookup):\n",
    "    definition = \"\"\"\n",
    "    gain_id: int  # gain id\n",
    "    ---\n",
    "    gain: float   # gain value\n",
    "    \"\"\"\n",
    "    contents = [(0, 1)]\n",
    "\n",
    "\n",
    "@schema\n",
    "class SimulationSeed(dj.Lookup):\n",
    "    definition = \"\"\"\n",
    "    sim_seed: int   # simulation seed\n",
    "    \"\"\"\n",
    "    contents = list(zip([114514]))\n",
    "\n",
    "@schema\n",
    "class PoissonSimulation(dj.Computed):\n",
    "    definition = \"\"\"\n",
    "    -> StimulusSession\n",
    "    -> GaussTuningSet\n",
    "    -> ResponseGain\n",
    "    -> SimulationSeed\n",
    "    \"\"\"\n",
    "\n",
    "    def make(self, key):\n",
    "        self.insert1(key)\n",
    "\n",
    "    def simulate_responses(self, key=None):\n",
    "        key = key or {}\n",
    "        key = (self & key).fetch1('KEY')\n",
    "        gain = (ResponseGain() & key).fetch1('gain')\n",
    "        sim_seed = (SimulationSeed() & key).fetch1('sim_seed')\n",
    "        stims = (StimulusSession() & key).get_stimulus()\n",
    "        stim_keys = ['train', 'valid', 'test']\n",
    "\n",
    "        # this is the expected value of the spike counts for each stimulus\n",
    "        f = (GaussTuningSet() & key).get_f()\n",
    "\n",
    "        responses = {\n",
    "            'mean_f': f\n",
    "        }\n",
    "\n",
    "        np.random.seed(sim_seed)\n",
    "        for k, stim in zip(stim_keys, stims):\n",
    "            resp_set = {}\n",
    "            mus = (f(stim) * gain).T # make it into trials x units\n",
    "            counts = np.random.poisson(mus)\n",
    "            def closure(counts):\n",
    "                def ll(decode_stim):\n",
    "                    mu_hat = (gain * f(decode_stim))[None, ...] # 1 x units x dec_stim\n",
    "                    logl = (counts[..., None] * np.log(mu_hat)).sum(axis=1) - mu_hat.sum(axis=1)   # trials x dec_stim\n",
    "                    logl = logl - logl.max(axis=1, keepdims=True)\n",
    "                    return logl\n",
    "                return ll\n",
    "\n",
    "            resp_set['stimulus'] = stim\n",
    "            resp_set['expected_responses'] = mus\n",
    "            resp_set['counts'] = counts\n",
    "            resp_set['logl_f'] = closure(counts)\n",
    "\n",
    "            responses[k] = resp_set\n",
    "\n",
    "        return responses\n",
    "\n",
    "@schema\n",
    "class BinConfig(dj.Lookup):\n",
    "    definition = \"\"\"\n",
    "    bin_config_id  : varchar(128)   # id\n",
    "    ---\n",
    "    bin_width: decimal(3, 2)\n",
    "    bin_counts: int  # number of bins\n",
    "    clip_outside: bool   # whether to clip outside\n",
    "    \"\"\"\n",
    "    contents = [\n",
    "        (list_hash(x),) + x for x in [\n",
    "            (1.0, 91, True)\n",
    "        ]\n",
    "    ]\n",
    "    \n",
    "    def get_binc(self, key=None):\n",
    "        if key is None:\n",
    "            key = self.fetch1('KEY')\n",
    "        width, counts = (self & key).fetch1('bin_width', 'bin_counts')\n",
    "        return (np.arange(counts) - counts//2) * width\n",
    "\n",
    "@schema\n",
    "class GTScores(dj.Computed):\n",
    "    definition = \"\"\"\n",
    "    -> PoissonSimulation\n",
    "    -> BinConfig\n",
    "    ---\n",
    "    gt_trainset_score: float \n",
    "    gt_validset_score: float\n",
    "    gt_testset_score: float\n",
    "    \"\"\"\n",
    "\n",
    "    def make(self, key):\n",
    "        resp = (PoissonSimulation() & key).simulate_responses()\n",
    "\n",
    "        delta, nbins, clip_outside = (BinConfig() & key).fetch1('bin_width', 'bin_counts', 'clip_outside')\n",
    "        delta = float(delta)\n",
    "\n",
    "        sigmaA = 3\n",
    "        sigmaB = 15\n",
    "\n",
    "        set_types = ['train', 'valid', 'test']\n",
    "\n",
    "        pv = (np.arange(nbins) - nbins // 2) * delta\n",
    "        prior = np.log(np.exp(- pv ** 2 / 2 / sigmaA ** 2) / sigmaA + np.exp(- pv ** 2 / 2 / sigmaB ** 2) / sigmaB)\n",
    "\n",
    "        for st in set_types:\n",
    "            logl = resp[st]['logl_f'](pv)\n",
    "            ori = resp[st]['stimulus']\n",
    "            xv, ori_bins = binnify(ori, delta=delta, nbins=nbins, clip=clip_outside)\n",
    "\n",
    "            y = logl + prior\n",
    "            t_hat = np.argmax(y, 1)\n",
    "\n",
    "            key['gt_{}set_score'.format(st)] = np.sqrt(np.mean((t_hat.ravel() -  ori_bins) ** 2)) * delta\n",
    "\n",
    "        self.insert1(key)\n",
    "\n",
    "\n",
    "def mse(y, t):\n",
    "    return np.sqrt(np.mean((y - t)**2))\n",
    "\n",
    "\n",
    "@schema\n",
    "class LinearRegression(dj.Computed):\n",
    "    definition = \"\"\"\n",
    "    -> BinConfig\n",
    "    -> PoissonSimulation\n",
    "    ---\n",
    "    lr_weights : longblob        # learned weights\n",
    "    lr_trainset_score:  float    # score on trainset \n",
    "    lr_validset_score:  float    # score on validation set\n",
    "    lr_testset_score:   float    # score on testset\n",
    "    \"\"\"\n",
    "\n",
    "    def make(self, key):\n",
    "        resp = (PoissonSimulation() & key).simulate_responses()\n",
    "\n",
    "        bin_width, bin_counts, clip_outside = (BinConfig() & key).fetch1('bin_width', 'bin_counts', 'clip_outside')\n",
    "        bin_width = float(bin_width)\n",
    "\n",
    "        train_counts, train_ori = resp['train']['counts'], resp['train']['stimulus']\n",
    "        valid_counts, valid_ori = resp['valid']['counts'], resp['valid']['stimulus']\n",
    "        \n",
    "        test_counts, test_ori = resp['test']['counts'], resp['test']['stimulus']\n",
    "        \n",
    "\n",
    "        xv, train_bins = binnify(train_ori, delta=bin_width, nbins=bin_counts, clip=clip_outside)\n",
    "        _, valid_bins = binnify(valid_ori, delta=bin_width, nbins=bin_counts, clip=clip_outside)\n",
    "        _, test_bins = binnify(test_ori, delta=bin_width, nbins=bin_counts, clip=clip_outside)\n",
    "\n",
    "        good_pos = train_bins >= 0\n",
    "        train_counts = train_counts[good_pos]\n",
    "        train_bins = train_bins[good_pos]\n",
    "\n",
    "        good_pos = valid_bins >= 0\n",
    "        valid_counts = valid_counts[good_pos]\n",
    "        valid_bins = valid_bins[good_pos]\n",
    "        \n",
    "        good_pos = test_bins >= 0\n",
    "        test_counts = test_counts[good_pos]\n",
    "        test_bins = test_bins[good_pos]\n",
    "        \n",
    "        grouped_counts = np.concatenate([train_counts, valid_counts])\n",
    "        grouped_bins = np.concatenate([train_bins, valid_bins])\n",
    "\n",
    "        gc = extend_ones(grouped_counts)\n",
    "        # plain old ridge linear regression\n",
    "        w = inv(gc.T @ gc + np.eye(gc.shape[1]) * 0.0001) @ gc.T @ grouped_bins\n",
    "\n",
    "        t_hat_train = extend_ones(train_counts) @ w\n",
    "        t_hat_valid = extend_ones(valid_counts) @ w\n",
    "        t_hat_test = extend_ones(test_counts) @ w\n",
    "\n",
    "        train_score = mse(t_hat_train, train_bins) * bin_width\n",
    "        valid_score = mse(t_hat_valid, valid_bins) * bin_width\n",
    "        test_score = mse(t_hat_test, test_bins) * bin_width\n",
    "\n",
    "        key['lr_weights'] = w\n",
    "        key['lr_trainset_score'] = train_score\n",
    "        key['lr_validset_score'] = valid_score\n",
    "        key['lr_testset_score'] = test_score\n",
    "\n",
    "        self.insert1(key)\n",
    "\n",
    "@schema\n",
    "class FitTuningCurves(dj.Computed):\n",
    "    definition = \"\"\"\n",
    "    -> PoissonSimulation\n",
    "    ---\n",
    "    fit_amps: longblob            # amplitude of tuning curves\n",
    "    fit_centers: longblob        # center of tuning curves\n",
    "    fit_widths: longblob         # width of tuning curves\n",
    "    \"\"\"\n",
    "\n",
    "    def make(self, key):\n",
    "        resp = (PoissonSimulation() & key).simulate_responses()\n",
    "        \n",
    "        counts = np.concatenate([resp['train']['counts'], resp['valid']['counts']])\n",
    "        ori = np.concatenate([resp['train']['stimulus'], resp['valid']['stimulus']])\n",
    "        ct = counts.T\n",
    "\n",
    "        mu_counts = (ct * ori).sum(axis=1, keepdims=True) / ct.sum(axis=1, keepdims=True)\n",
    "        sigma_counts = np.sqrt((ct * ori**2).sum(axis=1, keepdims=True) / ct.sum(axis=1, keepdims=True) - mu_counts**2)\n",
    "        max_counts = counts.max(axis=0)\n",
    "\n",
    "        from scipy.optimize import curve_fit\n",
    "        \n",
    "        def gaus(x,a,x0,sigma):\n",
    "            return a*np.exp(-(x-x0)**2/2/(sigma**2))\n",
    "\n",
    "        amps = []\n",
    "        centers = []\n",
    "        widths = []\n",
    "\n",
    "        for i in range(counts.shape[1]):\n",
    "            pp, po = curve_fit(gaus, ori, counts[:, i], p0=[max_counts[i], mu_counts[i], sigma_counts[i]])\n",
    "            amps.append(pp[0])\n",
    "            centers.append(pp[1])\n",
    "            widths.append(pp[2])\n",
    "\n",
    "            \n",
    "        key['fit_amps'] = np.array(amps)\n",
    "        key['fit_centers'] = np.array(centers)\n",
    "        key['fit_widths'] = np.array(widths)\n",
    "        \n",
    "        self.insert1(key)\n",
    "        \n",
    "    def get_f(self, key=None):\n",
    "        \"\"\"\n",
    "        Return a function that would return mean population response for stimuli.\n",
    "        Returns n_neurons x n_stimuli matrix\n",
    "        \"\"\"\n",
    "        key = key or {}\n",
    "        centers, amps, widths = (self & key).fetch1('fit_centers', 'fit_amps', 'fit_widths')\n",
    "        centers = centers[:, None]\n",
    "        amps = amps[:, None]\n",
    "        widths = widths[:, None]\n",
    "        def f(stim):\n",
    "            return np.exp(-(stim - centers)**2 / 2 / widths**2) * amps\n",
    "\n",
    "        return f\n",
    "    \n",
    "\n",
    "\n",
    "@schema\n",
    "class FittedPoissonScores(dj.Computed):\n",
    "    definition = \"\"\"\n",
    "    -> FitTuningCurves\n",
    "    -> BinConfig\n",
    "    ---\n",
    "    fit_trainset_score: float \n",
    "    fit_validset_score: float\n",
    "    fit_testset_score: float\n",
    "    \"\"\"\n",
    "    \n",
    "    def response_summary(self, key=None):\n",
    "        if key is None:\n",
    "            key = self.fetch1('KEY')\n",
    "\n",
    "        stim_keys = ['train', 'valid', 'test']\n",
    "\n",
    "        # this is the expected value of the spike counts for each stimulus\n",
    "        f = (FitTuningCurves() & key).get_f()\n",
    "        responses = (PoissonSimulation() & key).simulate_responses()\n",
    "\n",
    "        responses['mean_f'] = f\n",
    "\n",
    "        for k in stim_keys:\n",
    "            resp_set = responses[k]\n",
    "            mus = f(resp_set['stimulus']) \n",
    "            counts = resp_set['counts']\n",
    "            def closure(counts):\n",
    "                def ll(decode_stim):\n",
    "                    mu_hat = f(decode_stim)[None, ...] # 1 x units x dec_stim\n",
    "                    logl = (counts[..., None] * np.log(mu_hat)).sum(axis=1) - mu_hat.sum(axis=1)   # trials x dec_stim\n",
    "                    return logl\n",
    "                return ll\n",
    "\n",
    "            resp_set['expected_responses'] = mus\n",
    "            resp_set['logl_f'] = closure(counts)\n",
    "\n",
    "        return responses\n",
    "\n",
    "    def make(self, key):\n",
    "        resp = self.response_summary(key)\n",
    "\n",
    "        delta, nbins, clip_outside = (BinConfig() & key).fetch1('bin_width', 'bin_counts', 'clip_outside')\n",
    "        delta = float(delta)\n",
    "\n",
    "        sigmaA = 3\n",
    "        sigmaB = 15\n",
    "\n",
    "        set_types = ['train', 'valid', 'test']\n",
    "\n",
    "        pv = (np.arange(nbins) - nbins // 2) * delta\n",
    "        prior = np.log(np.exp(- pv ** 2 / 2 / sigmaA ** 2) / sigmaA + np.exp(- pv ** 2 / 2 / sigmaB ** 2) / sigmaB)\n",
    "\n",
    "        for st in set_types:\n",
    "            logl = resp[st]['logl_f'](pv)\n",
    "            ori = resp[st]['stimulus']\n",
    "            xv, ori_bins = binnify(ori, delta=delta, nbins=nbins, clip=clip_outside)\n",
    "\n",
    "            y = logl + prior\n",
    "            t_hat = np.argmax(y, 1)\n",
    "\n",
    "            key['fit_{}set_score'.format(st)] = np.sqrt(np.mean((t_hat.ravel() -  ori_bins) ** 2)) * delta\n",
    "\n",
    "        self.insert1(key)\n",
    "\n",
    "@schema\n",
    "class FittedPoissonKL(dj.Computed):\n",
    "    definition = \"\"\"\n",
    "    -> FittedPoissonScores\n",
    "    ---\n",
    "    fit_train_med_kl: float  # med KL\n",
    "    fit_valid_med_kl: float  # med KL\n",
    "    fit_test_med_kl: float  # med KL\n",
    "    fit_train_kl: longblob   # KL values\n",
    "    fit_valid_kl: longblob   # KL values\n",
    "    fit_test_kl: longblob   # KL values\n",
    "    \"\"\"\n",
    "    \n",
    "    def make(self, key):\n",
    "        gt_resp = (PoissonSimulation() & key).simulate_responses()\n",
    "        fit_resp = (FittedPoissonScores() & key).response_summary()\n",
    "\n",
    "        delta, nbins, clip_outside = (BinConfig() & key).fetch1('bin_width', 'bin_counts', 'clip_outside')\n",
    "        delta = float(delta)\n",
    "        set_types = ['train', 'valid', 'test']\n",
    "        pv = (np.arange(nbins) - nbins // 2) * delta\n",
    "\n",
    "        for st in set_types:\n",
    "            gt_logl = gt_resp[st]['logl_f'](pv)\n",
    "            gt_nl = np.exp(gt_logl)\n",
    "            gt_nl = gt_nl / np.sum(gt_nl, axis=1, keepdims=True)\n",
    "\n",
    "            fit_logl = fit_resp[st]['logl_f'](pv)\n",
    "            fit_nl = np.exp(fit_logl)\n",
    "            fit_nl = fit_nl / np.sum(fit_nl, axis=1, keepdims=True)\n",
    "\n",
    "            eps=1e-15\n",
    "            KL = ((np.log(gt_nl + eps) - np.log(fit_nl + eps)) * gt_nl).sum(axis=1)\n",
    "            key['fit_{}_med_kl'.format(st)] = np.median(KL)\n",
    "            key['fit_{}_kl'.format(st)] = KL\n",
    "            \n",
    "        self.insert1(key)\n",
    "\n",
    "@schema\n",
    "class TrainSeed(dj.Lookup):\n",
    "    definition = \"\"\"\n",
    "    # training seed\n",
    "    train_seed:   int       # training seed\n",
    "    \"\"\"\n",
    "    contents = zip((8, 92, 123))\n",
    "\n",
    "@schema\n",
    "class ModelDesign(dj.Lookup):\n",
    "    definition = \"\"\"\n",
    "    model_id: varchar(128)   # model id\n",
    "    ---\n",
    "    hidden1:  int      # size of first hidden layer\n",
    "    hidden2:  int      # size of second hidden layer\n",
    "    \"\"\"\n",
    "    contents = [(list_hash(x),) + x for x in [\n",
    "        (400, 400),\n",
    "        (600, 600),\n",
    "        (800, 800),\n",
    "    ]]\n",
    "\n",
    "\n",
    "@schema\n",
    "class TrainParam(dj.Lookup):\n",
    "    definition = \"\"\"\n",
    "    param_id: varchar(128)    # ID of parameter\n",
    "    ---\n",
    "    learning_rate:  float     # initial learning rate\n",
    "    dropout:       float     # dropout rate\n",
    "    init_std:       float     # standard deviation for weight initialization\n",
    "    smoothness:     float     # regularizer on Laplace smoothness\n",
    "    \"\"\"\n",
    "    contents = [(list_hash(x), ) + x for x in product(\n",
    "        (0.01, 0.03, 0.3),     # learning rate\n",
    "        (0.4, 0.5),      # dropout rate\n",
    "        (0.01, 0.001, 0.0001),    # initialization std\n",
    "        (3, 30, 300, 3000)  # smoothness\n",
    "    )]\n",
    "\n",
    "\n",
    "@schema\n",
    "class PoissonTrainedModel(dj.Computed):\n",
    "    definition = \"\"\"\n",
    "    -> PoissonSimulation\n",
    "    -> BinConfig\n",
    "    -> ModelDesign\n",
    "    -> TrainParam\n",
    "    -> TrainSeed\n",
    "    ---\n",
    "    cnn_train_score: float   # score on train set\n",
    "    cnn_valid_score:  float   # score on validation set\n",
    "    cnn_test_score: float     # score on test set\n",
    "    avg_sigma:   float   # average width of the likelihood functions\n",
    "    model: longblob  # saved model\n",
    "    \"\"\"\n",
    "\n",
    "    def load_model(self, key=None):\n",
    "        if key is None:\n",
    "            key = {}\n",
    "\n",
    "        rel = self & key\n",
    "\n",
    "        state_dict = rel.fetch1('model')\n",
    "        state_dict = {k: torch.from_numpy(state_dict[k][0]) for k in state_dict.dtype.names}\n",
    "\n",
    "        init_std = float((TrainParam() & rel).fetch1('init_std'))\n",
    "        dropout = float((TrainParam() & rel).fetch1('dropout'))\n",
    "        h1, h2 = [int(x) for x in (ModelDesign() & rel).fetch1('hidden1', 'hidden2')]\n",
    "        nbins = int((BinConfig() & rel).fetch1('bin_counts'))\n",
    "\n",
    "        net = Net(n_output=nbins, n_hidden=[h1, h2], std=init_std, dropout=dropout)\n",
    "        net.load_state_dict(state_dict)\n",
    "        return net\n",
    "\n",
    "    def get_dataset(self, key=None, keep_all=False):\n",
    "        if key is None:\n",
    "            key = self.fetch1(dj.key)\n",
    "\n",
    "        resp = (PoissonSimulation() & key).simulate_responses()\n",
    "        bin_width = float((BinConfig() & key).fetch1('bin_width'))\n",
    "        bin_counts = int((BinConfig() & key).fetch1('bin_counts'))\n",
    "        clip_outside = bool((BinConfig() & key).fetch1('clip_outside')) and not keep_all\n",
    "\n",
    "        train_counts, train_ori = resp['train']['counts'], resp['train']['stimulus']\n",
    "        valid_counts, valid_ori = resp['valid']['counts'], resp['valid']['stimulus']\n",
    "        test_counts, test_ori = resp['test']['counts'], resp['test']['stimulus']\n",
    "\n",
    "        xv, train_bins = binnify(train_ori, delta=bin_width, nbins=bin_counts, clip=clip_outside)\n",
    "        _, valid_bins = binnify(valid_ori, delta=bin_width, nbins=bin_counts, clip=clip_outside)\n",
    "        _, test_bins = binnify(test_ori, delta=bin_width, nbins=bin_counts, clip=clip_outside)\n",
    "\n",
    "        good_pos = train_bins >= 0\n",
    "        train_counts = train_counts[good_pos]\n",
    "        train_ori = train_bins[good_pos]\n",
    "\n",
    "        good_pos = valid_bins >= 0\n",
    "        valid_counts = valid_counts[good_pos]\n",
    "        valid_ori = valid_bins[good_pos]\n",
    "\n",
    "        good_pos = test_bins >= 0\n",
    "        test_counts = test_counts[good_pos]\n",
    "        test_ori = test_bins[good_pos]\n",
    "\n",
    "        train_x = torch.Tensor(train_counts)\n",
    "        train_t = torch.Tensor(train_ori).type(torch.LongTensor)\n",
    "\n",
    "        valid_x = Variable(torch.Tensor(valid_counts))\n",
    "        valid_t = Variable(torch.Tensor(valid_ori).type(torch.LongTensor))\n",
    "\n",
    "        test_x = Variable(torch.Tensor(test_counts))\n",
    "        test_t = Variable(torch.Tensor(test_ori).type(torch.LongTensor))\n",
    "\n",
    "        return train_x, train_t, valid_x, valid_t, test_x, test_t\n",
    "\n",
    "    def make_objective(self, valid_x, valid_t, prior, delta):\n",
    "        def objective(net, x=None, t=None):\n",
    "            if x is None and t is None:\n",
    "                x = valid_x\n",
    "                t = valid_t\n",
    "            net.eval()\n",
    "            y = net(x)\n",
    "            posterior = y + prior\n",
    "            _, loc = torch.max(posterior, dim=1)\n",
    "            v = (t.double() - loc.double()).pow(2).mean().sqrt() * delta\n",
    "            return v.data.cpu().numpy()[0]\n",
    "        return objective\n",
    "\n",
    "    def train(self, net, loss, objective, train_dataset, prior, alpha, init_lr):\n",
    "        learning_rates = init_lr * 3.0 ** (-np.arange(4))\n",
    "        for lr in learning_rates:\n",
    "            print('\\n\\n\\n\\n LEARNING RATE: {}'.format(lr))\n",
    "            optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "            for epoch, valid_score in early_stopping(net, objective, interval=20, start=100, patience=20,\n",
    "                                                     max_iter=300000, maximize=False):\n",
    "                data_loader = DataLoader(train_dataset, shuffle=True, batch_size=128)\n",
    "                for x_, t_ in data_loader:\n",
    "                    x, t = Variable(x_).cuda(), Variable(t_).cuda()\n",
    "                    net.train()\n",
    "                    optimizer.zero_grad()\n",
    "                    y = net(x)\n",
    "                    post = y + prior\n",
    "                    val, _ = post.max(1, keepdim=True)\n",
    "                    post = post - val\n",
    "                    conv_filter = Variable(\n",
    "                        torch.from_numpy(np.array([-0.25, 0.5, -0.25])[None, None, :]).type(y.data.type()))\n",
    "                    try:\n",
    "                        smoothness = nn.functional.conv1d(y.unsqueeze(1), conv_filter).pow(2).mean()\n",
    "                    except:\n",
    "                        # if smoothness computation overflows, then don't bother with it\n",
    "                        smoothness = 0\n",
    "                    score = loss(post, t)\n",
    "                    score = score + alpha * smoothness\n",
    "                    score.backward()\n",
    "                    optimizer.step()\n",
    "                if epoch % 10 == 0:\n",
    "                    print('Score: {}'.format(score.data.cpu().numpy()[0]))\n",
    "                    # scheduler.step()\n",
    "\n",
    "    def test_model(self, key=None):\n",
    "        if key is None:\n",
    "            key = self.fetch1('KEY')\n",
    "\n",
    "        net = self.load_model(key)\n",
    "        net.cuda()\n",
    "        net.eval()\n",
    "\n",
    "        objective = self.prepare_objective(key)\n",
    "        train_x, train_t, valid_x, valid_t, test_x, test_t = self.get_dataset(key)\n",
    "        valid_x, valid_t = valid_x.cuda(), valid_t.cuda()\n",
    "        test_x, test_t = test_x.cuda(), test_t.cuda()\n",
    "        train_score = objective(net, x=Variable(train_x).cuda(), t=Variable(train_t).cuda())\n",
    "        valid_score = objective(net, x=valid_x, t=valid_t)\n",
    "        test_score = objective(net, x=test_x, t=test_t)\n",
    "\n",
    "        return train_score, valid_score, test_score\n",
    "\n",
    "\n",
    "    def prepare_objective(self, key):\n",
    "        delta = float((BinConfig() & key).fetch1('bin_width'))\n",
    "        nbins = int((BinConfig() & key).fetch1('bin_counts'))\n",
    "\n",
    "        sigmaA = 3\n",
    "        sigmaB = 15\n",
    "        pv = (np.arange(nbins) - nbins // 2) * delta\n",
    "        prior = np.log(np.exp(- pv ** 2 / 2 / sigmaA ** 2) / sigmaA + np.exp(- pv ** 2 / 2 / sigmaB ** 2) / sigmaB)\n",
    "        prior = Variable(torch.from_numpy(prior)).cuda().float()\n",
    "\n",
    "        train_x, train_t, valid_x, valid_t, test_x, test_t = self.get_dataset(key)\n",
    "\n",
    "        valid_x, valid_t = valid_x.cuda(), valid_t.cuda()\n",
    "\n",
    "        return self.make_objective(valid_x, valid_t, prior, delta)\n",
    "\n",
    "\n",
    "    def make(self, key):\n",
    "        #train_counts, train_ori, valid_counts, valid_ori = self.get_dataset(key)\n",
    "\n",
    "        delta = float((BinConfig() & key).fetch1('bin_width'))\n",
    "        nbins = int((BinConfig() & key).fetch1('bin_counts'))\n",
    "\n",
    "        sigmaA = 3\n",
    "        sigmaB = 15\n",
    "        pv = (np.arange(nbins) - nbins // 2) * delta\n",
    "        prior = np.log(np.exp(- pv ** 2 / 2 / sigmaA ** 2) / sigmaA + np.exp(- pv ** 2 / 2 / sigmaB ** 2) / sigmaB)\n",
    "        prior = Variable(torch.from_numpy(prior)).cuda().float()\n",
    "\n",
    "        train_x, train_t, valid_x, valid_t, test_x, test_t = self.get_dataset(key)\n",
    "\n",
    "        valid_x, valid_t = valid_x.cuda(), valid_t.cuda()\n",
    "        test_x, test_t = test_x.cuda(), test_t.cuda()\n",
    "\n",
    "        train_dataset = TensorDataset(train_x, train_t)\n",
    "        #valid_dataset = TensorDataset(valid_x, valid_t)\n",
    "\n",
    "        objective = self.make_objective(valid_x, valid_t, prior, delta)\n",
    "\n",
    "\n",
    "        init_lr = float((TrainParam() & key).fetch1('learning_rate'))\n",
    "        alpha = float((TrainParam() & key).fetch1('smoothness'))\n",
    "        init_std = float((TrainParam() & key).fetch1('init_std'))\n",
    "        dropout = float((TrainParam() & key).fetch1('dropout'))\n",
    "        h1, h2 = [int(x) for x in (ModelDesign() & key).fetch1('hidden1', 'hidden2')]\n",
    "        seed = key['train_seed']\n",
    "\n",
    "        net = Net(n_output=nbins, n_hidden=[h1, h2], std=init_std, dropout=dropout)\n",
    "        net.cuda()\n",
    "        loss = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "        net.std = init_std\n",
    "        set_seed(seed)\n",
    "        net.initialize()\n",
    "\n",
    "        self.train(net, loss, objective, train_dataset, prior, alpha, init_lr)\n",
    "\n",
    "        print('Evaluating...')\n",
    "        net.eval()\n",
    "\n",
    "        key['cnn_train_score'] = objective(net, x=Variable(train_x).cuda(), t=Variable(train_t).cuda())\n",
    "        key['cnn_valid_score'] = objective(net, x=valid_x, t=valid_t)\n",
    "        key['cnn_test_score'] = objective(net, x=test_x, t=test_t)\n",
    "\n",
    "\n",
    "        y = net(test_x)\n",
    "        yd = y.data.cpu().numpy()\n",
    "        yd = np.exp(yd)\n",
    "        yd = yd / yd.sum(axis=1, keepdims=True)\n",
    "\n",
    "        loc = yd.argmax(axis=1)\n",
    "        ds = (np.arange(nbins) - loc[:, None]) ** 2\n",
    "        avg_sigma = np.mean(np.sqrt(np.sum(yd * ds, axis=1))) * delta\n",
    "        if np.isnan(avg_sigma):\n",
    "            avg_sigma = -1\n",
    "\n",
    "        key['avg_sigma'] = avg_sigma\n",
    "        key['model'] = {k: v.cpu().numpy() for k, v in net.state_dict().items()}\n",
    "\n",
    "        self.insert1(key)\n",
    "\n",
    "@schema\n",
    "class TrainedNetKL(dj.Computed):\n",
    "    definition = \"\"\"\n",
    "    -> PoissonTrainedModel\n",
    "    ---\n",
    "    cnn_train_med_kl: float  # med KL\n",
    "    cnn_valid_med_kl: float  # med KL\n",
    "    cnn_test_med_kl: float  # med KL\n",
    "    cnn_train_kl: longblob   # KL values\n",
    "    cnn_valid_kl: longblob   # KL values\n",
    "    cnn_test_kl: longblob   # KL values\n",
    "    \"\"\"\n",
    "    \n",
    "    def make(self, key):\n",
    "        gt_resp = (PoissonSimulation() & key).simulate_responses()\n",
    "        rel = (PoissonTrainedModel() & key)\n",
    "        model = rel.load_model()\n",
    "        train_x, train_t, valid_x, valid_t, test_x, test_t = rel.get_dataset(keep_all=True)\n",
    "        train_x, train_t = Variable(train_x), Variable(train_t)\n",
    "        \n",
    "        model.cuda()\n",
    "        model.eval()\n",
    "        \n",
    "        delta, nbins, clip_outside = (BinConfig() & key).fetch1('bin_width', 'bin_counts', 'clip_outside')\n",
    "        delta = float(delta)\n",
    "        pv = (np.arange(nbins) - nbins // 2) * delta\n",
    "        \n",
    "        \n",
    "        set_types = ['train', 'valid', 'test']\n",
    "        set_x = [train_x, valid_x, test_x]\n",
    "        \n",
    "\n",
    "        for st, x in zip(set_types, set_x):\n",
    "            gt_logl = gt_resp[st]['logl_f'](pv)\n",
    "            gt_nl = np.exp(gt_logl)\n",
    "            gt_nl = gt_nl / np.sum(gt_nl, axis=1, keepdims=True)\n",
    "\n",
    "            cnn_logl = model(x.cuda()).data.cpu().numpy()\n",
    "            cnn_nl = np.exp(cnn_logl)\n",
    "            cnn_nl = cnn_nl / np.sum(cnn_nl, axis=1, keepdims=True)\n",
    "\n",
    "            eps=1e-15\n",
    "            KL = ((np.log(gt_nl + eps) - np.log(cnn_nl + eps)) * gt_nl).sum(axis=1)\n",
    "            key['cnn_{}_med_kl'.format(st)] = np.median(KL)\n",
    "            key['cnn_{}_kl'.format(st)] = KL\n",
    "            \n",
    "        self.insert1(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
